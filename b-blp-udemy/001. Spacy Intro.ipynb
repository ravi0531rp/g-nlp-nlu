{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2b7c326",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "     |████████████████████████████████| 13.9 MB 1.8 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from en-core-web-sm==3.2.0) (3.2.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.0)\n",
      "Requirement already satisfied: setuptools in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (58.5.3)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.12)\n",
      "Requirement already satisfied: jinja2 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.6)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.25.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.21.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.8.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.62.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.7)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.2.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3d33520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy-lookups-data\n",
      "  Downloading spacy_lookups_data-1.0.3-py2.py3-none-any.whl (98.5 MB)\n",
      "     |████████████████████████████████| 98.5 MB 5.9 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy-lookups-data) (58.5.3)\n",
      "Installing collected packages: spacy-lookups-data\n",
      "Successfully installed spacy-lookups-data-1.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy-lookups-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55c16f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy-transformers\n",
      "  Downloading spacy_transformers-1.1.2-py2.py3-none-any.whl (51 kB)\n",
      "     |████████████████████████████████| 51 kB 133 kB/s             \n",
      "\u001b[?25hCollecting spacy-alignments<1.0.0,>=0.7.2\n",
      "  Downloading spacy_alignments-0.8.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "     |████████████████████████████████| 1.1 MB 2.1 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: spacy<4.0.0,>=3.1.3 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy-transformers) (3.2.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.0 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy-transformers) (2.4.2)\n",
      "Collecting transformers<4.12.0,>=3.4.0\n",
      "  Downloading transformers-4.11.3-py3-none-any.whl (2.9 MB)\n",
      "     |████████████████████████████████| 2.9 MB 33.7 MB/s            \n",
      "\u001b[?25hCollecting torch>=1.6.0\n",
      "  Downloading torch-1.10.0-cp38-cp38-manylinux1_x86_64.whl (881.9 MB)\n",
      "     |████████████████████████████████| 881.9 MB 3.3 kB/s            \n",
      "\u001b[?25hRequirement already satisfied: pathy>=0.3.5 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers) (0.6.1)\n",
      "Requirement already satisfied: setuptools in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers) (58.5.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers) (3.0.6)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers) (1.8.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers) (2.0.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers) (3.3.0)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers) (8.0.12)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers) (1.0.6)\n",
      "Requirement already satisfied: jinja2 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers) (3.0.3)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers) (0.4.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers) (2.25.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers) (1.0.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers) (2.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers) (21.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers) (0.7.5)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers) (0.8.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers) (3.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers) (4.62.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from spacy<4.0.0,>=3.1.3->spacy-transformers) (1.21.4)\n",
      "Requirement already satisfied: typing-extensions in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from torch>=1.6.0->spacy-transformers) (3.10.0.2)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.4.0-py3-none-any.whl (9.8 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from transformers<4.12.0,>=3.4.0->spacy-transformers) (2021.8.3)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "     |████████████████████████████████| 3.3 MB 716 kB/s            \n",
      "\u001b[?25hCollecting sacremoses\n",
      "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
      "     |████████████████████████████████| 895 kB 4.0 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from transformers<4.12.0,>=3.4.0->spacy-transformers) (6.0)\n",
      "Collecting huggingface-hub>=0.0.17\n",
      "  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n",
      "     |████████████████████████████████| 59 kB 2.6 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from packaging>=20.0->spacy<4.0.0,>=3.1.3->spacy-transformers) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from pathy>=0.3.5->spacy<4.0.0,>=3.1.3->spacy-transformers) (5.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.1.3->spacy-transformers) (2021.10.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.1.3->spacy-transformers) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.1.3->spacy-transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.1.3->spacy-transformers) (1.26.7)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from typer<0.5.0,>=0.3.0->spacy<4.0.0,>=3.1.3->spacy-transformers) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from jinja2->spacy<4.0.0,>=3.1.3->spacy-transformers) (2.0.1)\n",
      "Requirement already satisfied: joblib in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from sacremoses->transformers<4.12.0,>=3.4.0->spacy-transformers) (1.1.0)\n",
      "Requirement already satisfied: six in /home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages (from sacremoses->transformers<4.12.0,>=3.4.0->spacy-transformers) (1.16.0)\n",
      "Installing collected packages: filelock, tokenizers, sacremoses, huggingface-hub, transformers, torch, spacy-alignments, spacy-transformers\n",
      "Successfully installed filelock-3.4.0 huggingface-hub-0.1.2 sacremoses-0.0.46 spacy-alignments-0.8.4 spacy-transformers-1.1.2 tokenizers-0.10.3 torch-1.10.0 transformers-4.11.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcecfda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbc7176e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'001. Spacy Intro.ipynb'   requirements.txt\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04e3beeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absl-py @ file:///home/conda/feedstock_root/build_artifacts/absl-py_1634676905105/work\r\n",
      "aiohttp @ file:///home/conda/feedstock_root/build_artifacts/aiohttp_1636085297963/work\r\n",
      "alabaster @ file:///home/ktietz/src/ci/alabaster_1611921544520/work\r\n",
      "appdirs==1.4.4\r\n",
      "argh==0.26.2\r\n",
      "argon2-cffi @ file:///home/conda/feedstock_root/build_artifacts/argon2-cffi_1636021380558/work\r\n",
      "arrow==0.13.1\r\n",
      "astor @ file:///home/conda/feedstock_root/build_artifacts/astor_1593610464257/work\r\n",
      "astroid @ file:///tmp/build/80754af9/astroid_1613500854201/work\r\n",
      "astunparse @ file:///home/conda/feedstock_root/build_artifacts/astunparse_1610696312422/work\r\n",
      "async-generator @ file:///home/ktietz/src/ci/async_generator_1611927993394/work\r\n",
      "async-timeout==3.0.1\r\n",
      "atomicwrites==1.4.0\r\n",
      "attrs @ file:///home/conda/feedstock_root/build_artifacts/attrs_1620387926260/work\r\n",
      "autopep8 @ file:///tmp/build/80754af9/autopep8_1620866417880/work\r\n",
      "Babel @ file:///tmp/build/80754af9/babel_1620871417480/work\r\n",
      "backcall @ file:///home/ktietz/src/ci/backcall_1611930011877/work\r\n",
      "binaryornot @ file:///tmp/build/80754af9/binaryornot_1617751525010/work\r\n",
      "black==19.10b0\r\n",
      "bleach @ file:///tmp/build/80754af9/bleach_1628110601003/work\r\n",
      "blinker==1.4\r\n",
      "blis @ file:///home/conda/feedstock_root/build_artifacts/cython-blis_1636053204017/work\r\n",
      "brotlipy @ file:///home/conda/feedstock_root/build_artifacts/brotlipy_1636012188166/work\r\n",
      "cachetools @ file:///home/conda/feedstock_root/build_artifacts/cachetools_1633010882559/work\r\n",
      "catalogue @ file:///home/conda/feedstock_root/build_artifacts/catalogue_1632148445745/work\r\n",
      "certifi==2021.10.8\r\n",
      "cffi @ file:///tmp/build/80754af9/cffi_1625807838443/work\r\n",
      "chardet @ file:///home/conda/feedstock_root/build_artifacts/chardet_1635814844635/work\r\n",
      "click @ file:///home/conda/feedstock_root/build_artifacts/click_1635822600067/work\r\n",
      "cloudpickle @ file:///tmp/build/80754af9/cloudpickle_1632508026186/work\r\n",
      "colorama @ file:///tmp/build/80754af9/colorama_1607707115595/work\r\n",
      "cookiecutter @ file:///tmp/build/80754af9/cookiecutter_1617748928239/work\r\n",
      "cryptography @ file:///home/conda/feedstock_root/build_artifacts/cryptography_1636040645081/work\r\n",
      "cycler @ file:///home/conda/feedstock_root/build_artifacts/cycler_1635519461629/work\r\n",
      "cymem @ file:///home/conda/feedstock_root/build_artifacts/cymem_1636053152744/work\r\n",
      "dataclasses @ file:///home/conda/feedstock_root/build_artifacts/dataclasses_1628958434797/work\r\n",
      "debugpy @ file:///tmp/build/80754af9/debugpy_1629222698064/work\r\n",
      "decorator @ file:///tmp/build/80754af9/decorator_1632776554403/work\r\n",
      "defusedxml @ file:///tmp/build/80754af9/defusedxml_1615228127516/work\r\n",
      "diff-match-patch @ file:///Users/ktietz/demo/mc3/conda-bld/diff-match-patch_1630511840874/work\r\n",
      "docutils @ file:///tmp/build/80754af9/docutils_1620827984873/work\r\n",
      "en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl\r\n",
      "entrypoints==0.3\r\n",
      "filelock==3.4.0\r\n",
      "flake8 @ file:///tmp/build/80754af9/flake8_1620776156532/work\r\n",
      "Flask==2.0.2\r\n",
      "gast==0.3.3\r\n",
      "google-auth @ file:///home/conda/feedstock_root/build_artifacts/google-auth_1635863830555/work\r\n",
      "google-auth-oauthlib @ file:///home/conda/feedstock_root/build_artifacts/google-auth-oauthlib_1630497468950/work\r\n",
      "google-pasta==0.2.0\r\n",
      "grpcio @ file:///home/conda/feedstock_root/build_artifacts/grpcio_1635853271029/work\r\n",
      "h5py @ file:///home/conda/feedstock_root/build_artifacts/h5py_1617739432869/work\r\n",
      "huggingface-hub==0.1.2\r\n",
      "idna @ file:///home/conda/feedstock_root/build_artifacts/idna_1593328102638/work\r\n",
      "imagesize @ file:///home/ktietz/src/ci/imagesize_1611921604382/work\r\n",
      "importlib-metadata @ file:///home/conda/feedstock_root/build_artifacts/importlib-metadata_1636431522037/work\r\n",
      "inflection==0.5.1\r\n",
      "intervaltree @ file:///Users/ktietz/demo/mc3/conda-bld/intervaltree_1630511889664/work\r\n",
      "ipykernel @ file:///tmp/build/80754af9/ipykernel_1633545412716/work/dist/ipykernel-6.4.1-py3-none-any.whl\r\n",
      "ipython @ file:///tmp/build/80754af9/ipython_1635943998418/work\r\n",
      "ipython-genutils @ file:///tmp/build/80754af9/ipython_genutils_1606773439826/work\r\n",
      "isort @ file:///tmp/build/80754af9/isort_1628603791788/work\r\n",
      "itsdangerous==2.0.1\r\n",
      "jedi==0.17.2\r\n",
      "jeepney @ file:///tmp/build/80754af9/jeepney_1627537048313/work\r\n",
      "Jinja2==3.0.3\r\n",
      "jinja2-time @ file:///tmp/build/80754af9/jinja2-time_1617751524098/work\r\n",
      "joblib==1.1.0\r\n",
      "jsonschema @ file:///Users/ktietz/demo/mc3/conda-bld/jsonschema_1630511932244/work\r\n",
      "jupyter-client @ file:///tmp/build/80754af9/jupyter_client_1616770841739/work\r\n",
      "jupyter-core @ file:///tmp/build/80754af9/jupyter_core_1636524756443/work\r\n",
      "jupyter-tabnine==1.2.3\r\n",
      "jupyterlab-pygments @ file:///tmp/build/80754af9/jupyterlab_pygments_1601490720602/work\r\n",
      "jupyterthemes==0.20.0\r\n",
      "Keras==2.3.1\r\n",
      "Keras-Applications @ file:///tmp/build/80754af9/keras-applications_1594366238411/work\r\n",
      "Keras-Preprocessing @ file:///home/conda/feedstock_root/build_artifacts/keras-preprocessing_1610713559828/work\r\n",
      "keyring @ file:///tmp/build/80754af9/keyring_1614616740399/work\r\n",
      "kiwisolver @ file:///home/conda/feedstock_root/build_artifacts/kiwisolver_1635836691365/work\r\n",
      "langcodes @ file:///home/conda/feedstock_root/build_artifacts/langcodes_1636741340529/work\r\n",
      "lazy-object-proxy @ file:///tmp/build/80754af9/lazy-object-proxy_1616526917483/work\r\n",
      "lesscpy==0.15.0\r\n",
      "Markdown @ file:///home/conda/feedstock_root/build_artifacts/markdown_1614595805172/work\r\n",
      "MarkupSafe==2.0.1\r\n",
      "matplotlib @ file:///home/conda/feedstock_root/build_artifacts/matplotlib-suite_1632416634429/work\r\n",
      "matplotlib-inline @ file:///tmp/build/80754af9/matplotlib-inline_1628242447089/work\r\n",
      "mccabe==0.6.1\r\n",
      "mistune==0.8.4\r\n",
      "multidict @ file:///home/conda/feedstock_root/build_artifacts/multidict_1636019077110/work\r\n",
      "murmurhash @ file:///home/conda/feedstock_root/build_artifacts/murmurhash_1636019583024/work\r\n",
      "mypy-extensions==0.4.3\r\n",
      "nb-conda-kernels @ file:///home/conda/feedstock_root/build_artifacts/nb_conda_kernels_1606762469982/work\r\n",
      "nbclient @ file:///tmp/build/80754af9/nbclient_1614364831625/work\r\n",
      "nbconvert @ file:///tmp/build/80754af9/nbconvert_1624479060632/work\r\n",
      "nbformat @ file:///tmp/build/80754af9/nbformat_1617383369282/work\r\n",
      "nest-asyncio @ file:///tmp/build/80754af9/nest-asyncio_1613680548246/work\r\n",
      "notebook @ file:///home/conda/feedstock_root/build_artifacts/notebook_1634668519289/work\r\n",
      "numpy @ file:///home/conda/feedstock_root/build_artifacts/numpy_1636145332760/work\r\n",
      "numpydoc @ file:///tmp/build/80754af9/numpydoc_1605117425582/work\r\n",
      "oauthlib @ file:///home/conda/feedstock_root/build_artifacts/oauthlib_1622563202229/work\r\n",
      "olefile @ file:///home/conda/feedstock_root/build_artifacts/olefile_1602866521163/work\r\n",
      "opencv-python==4.5.4.58\r\n",
      "opt-einsum @ file:///home/conda/feedstock_root/build_artifacts/opt_einsum_1617859230218/work\r\n",
      "packaging @ file:///tmp/build/80754af9/packaging_1625611678980/work\r\n",
      "pandas @ file:///tmp/build/80754af9/pandas_1602088120436/work\r\n",
      "pandocfilters @ file:///tmp/build/80754af9/pandocfilters_1605120460739/work\r\n",
      "parso==0.7.1\r\n",
      "pathspec==0.7.0\r\n",
      "pathy @ file:///home/conda/feedstock_root/build_artifacts/pathy_1635227809952/work\r\n",
      "patsy==0.5.2\r\n",
      "pexpect @ file:///tmp/build/80754af9/pexpect_1605563209008/work\r\n",
      "pickleshare @ file:///tmp/build/80754af9/pickleshare_1606932040724/work\r\n",
      "Pillow @ file:///home/conda/feedstock_root/build_artifacts/pillow_1636558800772/work\r\n",
      "pluggy @ file:///tmp/build/80754af9/pluggy_1633715052817/work\r\n",
      "ply==3.11\r\n",
      "poyo @ file:///tmp/build/80754af9/poyo_1617751526755/work\r\n",
      "preshed @ file:///home/conda/feedstock_root/build_artifacts/preshed_1636077712344/work\r\n",
      "prometheus-client @ file:///home/conda/feedstock_root/build_artifacts/prometheus_client_1635538335951/work\r\n",
      "prompt-toolkit @ file:///tmp/build/80754af9/prompt-toolkit_1633440160888/work\r\n",
      "protobuf==3.19.1\r\n",
      "psutil @ file:///tmp/build/80754af9/psutil_1612298023621/work\r\n",
      "ptyprocess @ file:///tmp/build/80754af9/ptyprocess_1609355006118/work/dist/ptyprocess-0.7.0-py2.py3-none-any.whl\r\n",
      "pyasn1==0.4.8\r\n",
      "pyasn1-modules==0.2.7\r\n",
      "pycodestyle @ file:///tmp/build/80754af9/pycodestyle_1615748559966/work\r\n",
      "pycparser @ file:///home/conda/feedstock_root/build_artifacts/pycparser_1636257122734/work\r\n",
      "pydantic @ file:///home/conda/feedstock_root/build_artifacts/pydantic_1636021149719/work\r\n",
      "pydocstyle @ file:///tmp/build/80754af9/pydocstyle_1621600989141/work\r\n",
      "pyflakes @ file:///tmp/build/80754af9/pyflakes_1617200973297/work\r\n",
      "Pygments @ file:///tmp/build/80754af9/pygments_1629234116488/work\r\n",
      "PyJWT @ file:///home/conda/feedstock_root/build_artifacts/pyjwt_1634405536383/work\r\n",
      "pylint @ file:///tmp/build/80754af9/pylint_1617135829881/work\r\n",
      "pyls-spyder==0.4.0\r\n",
      "pyOpenSSL @ file:///home/conda/feedstock_root/build_artifacts/pyopenssl_1633192417276/work\r\n",
      "pyparsing @ file:///tmp/build/80754af9/pyparsing_1635766073266/work\r\n",
      "pyrsistent @ file:///tmp/build/80754af9/pyrsistent_1636110947380/work\r\n",
      "PySocks @ file:///home/conda/feedstock_root/build_artifacts/pysocks_1635862404924/work\r\n",
      "python-dateutil @ file:///tmp/build/80754af9/python-dateutil_1626374649649/work\r\n",
      "python-jsonrpc-server==0.4.0\r\n",
      "python-language-server==0.36.2\r\n",
      "python-lsp-black @ file:///tmp/build/80754af9/python-lsp-black_1634232156041/work\r\n",
      "python-lsp-jsonrpc==1.0.0\r\n",
      "python-lsp-server==1.2.4\r\n",
      "python-slugify @ file:///tmp/build/80754af9/python-slugify_1620405669636/work\r\n",
      "pytz==2021.3\r\n",
      "pyu2f @ file:///home/conda/feedstock_root/build_artifacts/pyu2f_1604248910016/work\r\n",
      "pyxdg @ file:///tmp/build/80754af9/pyxdg_1603822279816/work\r\n",
      "PyYAML==6.0\r\n",
      "pyzmq @ file:///tmp/build/80754af9/pyzmq_1628276010766/work\r\n",
      "QDarkStyle @ file:///tmp/build/80754af9/qdarkstyle_1617386714626/work\r\n",
      "qstylizer @ file:///tmp/build/80754af9/qstylizer_1617713584600/work/dist/qstylizer-0.1.10-py2.py3-none-any.whl\r\n",
      "QtAwesome @ file:///tmp/build/80754af9/qtawesome_1615991616277/work\r\n",
      "qtconsole @ file:///tmp/build/80754af9/qtconsole_1632739723211/work\r\n",
      "QtPy @ file:///tmp/build/80754af9/qtpy_1629397026935/work\r\n",
      "regex @ file:///tmp/build/80754af9/regex_1629301332491/work\r\n",
      "requests @ file:///home/conda/feedstock_root/build_artifacts/requests_1608156231189/work\r\n",
      "requests-oauthlib @ file:///home/conda/feedstock_root/build_artifacts/requests-oauthlib_1595492159598/work\r\n",
      "rope @ file:///tmp/build/80754af9/rope_1623703006312/work\r\n",
      "rsa @ file:///home/conda/feedstock_root/build_artifacts/rsa_1614171254180/work\r\n",
      "Rtree @ file:///tmp/build/80754af9/rtree_1618420845272/work\r\n",
      "sacremoses==0.0.46\r\n",
      "scikit-learn==1.0.1\r\n",
      "scipy @ file:///home/conda/feedstock_root/build_artifacts/scipy_1636410384154/work\r\n",
      "seaborn @ file:///tmp/build/80754af9/seaborn_1600553570093/work\r\n",
      "SecretStorage @ file:///tmp/build/80754af9/secretstorage_1614022784285/work\r\n",
      "Send2Trash @ file:///home/conda/feedstock_root/build_artifacts/send2trash_1628511208346/work\r\n",
      "shellingham @ file:///home/conda/feedstock_root/build_artifacts/shellingham_1612179560728/work\r\n",
      "sip==4.19.13\r\n",
      "six @ file:///home/conda/feedstock_root/build_artifacts/six_1620240208055/work\r\n",
      "smart-open @ file:///home/conda/feedstock_root/build_artifacts/smart_open_1630238320325/work\r\n",
      "snowballstemmer @ file:///tmp/build/80754af9/snowballstemmer_1611258885636/work\r\n",
      "sortedcontainers @ file:///tmp/build/80754af9/sortedcontainers_1623949099177/work\r\n",
      "spacy @ file:///home/conda/feedstock_root/build_artifacts/spacy_1636396211810/work\r\n",
      "spacy-alignments==0.8.4\r\n",
      "spacy-legacy @ file:///home/conda/feedstock_root/build_artifacts/spacy-legacy_1625687473390/work\r\n",
      "spacy-loggers @ file:///home/conda/feedstock_root/build_artifacts/spacy-loggers_1634809367310/work\r\n",
      "spacy-lookups-data==1.0.3\r\n",
      "spacy-transformers==1.1.2\r\n",
      "Sphinx==4.2.0\r\n",
      "sphinxcontrib-applehelp @ file:///home/ktietz/src/ci/sphinxcontrib-applehelp_1611920841464/work\r\n",
      "sphinxcontrib-devhelp @ file:///home/ktietz/src/ci/sphinxcontrib-devhelp_1611920923094/work\r\n",
      "sphinxcontrib-htmlhelp @ file:///tmp/build/80754af9/sphinxcontrib-htmlhelp_1623945626792/work\r\n",
      "sphinxcontrib-jsmath @ file:///home/ktietz/src/ci/sphinxcontrib-jsmath_1611920942228/work\r\n",
      "sphinxcontrib-qthelp @ file:///home/ktietz/src/ci/sphinxcontrib-qthelp_1611921055322/work\r\n",
      "sphinxcontrib-serializinghtml @ file:///tmp/build/80754af9/sphinxcontrib-serializinghtml_1624451540180/work\r\n",
      "spyder @ file:///tmp/build/80754af9/spyder_1636480225430/work\r\n",
      "spyder-kernels @ file:///tmp/build/80754af9/spyder-kernels_1634236926649/work\r\n",
      "srsly @ file:///home/conda/feedstock_root/build_artifacts/srsly_1635234043838/work\r\n",
      "statsmodels==0.13.1\r\n",
      "tensorboard @ file:///home/conda/feedstock_root/build_artifacts/tensorboard_1636578784510/work/tensorboard-2.7.0-py3-none-any.whl\r\n",
      "tensorboard-data-server @ file:///home/conda/feedstock_root/build_artifacts/tensorboard-data-server_1636045749270/work/tensorboard_data_server-0.6.0-py3-none-manylinux2010_x86_64.whl\r\n",
      "tensorboard-plugin-wit @ file:///home/conda/feedstock_root/build_artifacts/tensorboard-plugin-wit_1611075653546/work/tensorboard_plugin_wit-1.8.0-py3-none-any.whl\r\n",
      "tensorflow==2.2.0\r\n",
      "tensorflow-estimator @ file:///home/conda/feedstock_root/build_artifacts/tensorflow-split_1632571320808/work/tensorflow-estimator/wheel_dir/tensorflow_estimator-2.6.0-py2.py3-none-any.whl\r\n",
      "termcolor==1.1.0\r\n",
      "terminado @ file:///home/conda/feedstock_root/build_artifacts/terminado_1636052221351/work\r\n",
      "testpath @ file:///tmp/build/80754af9/testpath_1624638946665/work\r\n",
      "text-unidecode @ file:///Users/ktietz/demo/mc3/conda-bld/text-unidecode_1629401354553/work\r\n",
      "textdistance @ file:///tmp/build/80754af9/textdistance_1612461398012/work\r\n",
      "thinc @ file:///home/conda/feedstock_root/build_artifacts/thinc_1635495440554/work\r\n",
      "threadpoolctl==3.0.0\r\n",
      "three-merge @ file:///tmp/build/80754af9/three-merge_1607553261110/work\r\n",
      "tinycss @ file:///tmp/build/80754af9/tinycss_1617713798712/work\r\n",
      "tokenizers==0.10.3\r\n",
      "toml @ file:///tmp/build/80754af9/toml_1616166611790/work\r\n",
      "torch==1.10.0\r\n",
      "tornado @ file:///tmp/build/80754af9/tornado_1606942300299/work\r\n",
      "tqdm @ file:///home/conda/feedstock_root/build_artifacts/tqdm_1632160078689/work\r\n",
      "traitlets @ file:///tmp/build/80754af9/traitlets_1632522747050/work\r\n",
      "transformers==4.11.3\r\n",
      "typed-ast @ file:///tmp/build/80754af9/typed-ast_1624953673417/work\r\n",
      "typer @ file:///home/conda/feedstock_root/build_artifacts/typer_1630326630489/work\r\n",
      "typing-extensions @ file:///home/conda/feedstock_root/build_artifacts/typing_extensions_1632313171031/work\r\n",
      "ujson @ file:///tmp/build/80754af9/ujson_1611259522456/work\r\n",
      "Unidecode @ file:///tmp/build/80754af9/unidecode_1614712377438/work\r\n",
      "urllib3 @ file:///home/conda/feedstock_root/build_artifacts/urllib3_1632350318291/work\r\n",
      "wasabi @ file:///home/conda/feedstock_root/build_artifacts/wasabi_1612156086016/work\r\n",
      "watchdog @ file:///tmp/build/80754af9/watchdog_1624948585604/work\r\n",
      "wcwidth @ file:///Users/ktietz/demo/mc3/conda-bld/wcwidth_1629357192024/work\r\n",
      "webencodings==0.5.1\r\n",
      "Werkzeug @ file:///home/conda/feedstock_root/build_artifacts/werkzeug_1621518206714/work\r\n",
      "whichcraft @ file:///tmp/build/80754af9/whichcraft_1617751293875/work\r\n",
      "wrapt @ file:///home/conda/feedstock_root/build_artifacts/wrapt_1635836386246/work\r\n",
      "wurlitzer @ file:///tmp/build/80754af9/wurlitzer_1626947794172/work\r\n",
      "yapf @ file:///tmp/build/80754af9/yapf_1615749224965/work\r\n",
      "yarl @ file:///home/conda/feedstock_root/build_artifacts/yarl_1636046814972/work\r\n",
      "zipp @ file:///home/conda/feedstock_root/build_artifacts/zipp_1633302054558/work\r\n"
     ]
    }
   ],
   "source": [
    "cat requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0139cf9d",
   "metadata": {},
   "source": [
    "# Spacy Flow\n",
    "\n",
    "TEXT --> tokenizer  -->  parser  --> ner -->        ------> Doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9db68b",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "893cbe97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72b05efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d87b0880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on English in module spacy.lang.en object:\n",
      "\n",
      "class English(spacy.language.Language)\n",
      " |  English(vocab: Union[spacy.vocab.Vocab, bool] = True, *, max_length: int = 1000000, meta: Dict[str, Any] = {}, create_tokenizer: Union[Callable[[ForwardRef('Language')], Callable[[str], spacy.tokens.doc.Doc]], NoneType] = None, batch_size: int = 1000, **kwargs) -> None\n",
      " |  \n",
      " |  A text-processing pipeline. Usually you'll load this once per process,\n",
      " |  and pass the instance around your application.\n",
      " |  \n",
      " |  Defaults (class): Settings, data and factory methods for creating the `nlp`\n",
      " |      object and processing pipeline.\n",
      " |  lang (str): IETF language code, such as 'en'.\n",
      " |  \n",
      " |  DOCS: https://spacy.io/api/language\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      English\n",
      " |      spacy.language.Language\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  Defaults = <class 'spacy.lang.en.EnglishDefaults'>\n",
      " |      Language data defaults, available via Language.Defaults. Can be\n",
      " |      overwritten by language subclasses by defining their own subclasses of\n",
      " |      Language.Defaults.\n",
      " |  \n",
      " |  default_config = {'paths': {'train': None, 'dev': None, 'vectors'...s'...\n",
      " |  \n",
      " |  factories = {'attribute_ruler': <function make_attribute_rul...r': <fu...\n",
      " |  \n",
      " |  lang = 'en'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from spacy.language.Language:\n",
      " |  \n",
      " |  __call__(self, text: Union[str, spacy.tokens.doc.Doc], *, disable: Iterable[str] = [], component_cfg: Union[Dict[str, Dict[str, Any]], NoneType] = None) -> spacy.tokens.doc.Doc\n",
      " |      Apply the pipeline to some text. The text can span multiple sentences,\n",
      " |      and can contain arbitrary whitespace. Alignment into the original string\n",
      " |      is preserved.\n",
      " |      \n",
      " |      text (Union[str, Doc]): If `str`, the text to be processed. If `Doc`,\n",
      " |          the doc will be passed directly to the pipeline, skipping\n",
      " |          `Language.make_doc`.\n",
      " |      disable (List[str]): Names of the pipeline components to disable.\n",
      " |      component_cfg (Dict[str, dict]): An optional dictionary with extra\n",
      " |          keyword arguments for specific components.\n",
      " |      RETURNS (Doc): A container for accessing the annotations.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#call\n",
      " |  \n",
      " |  __init__(self, vocab: Union[spacy.vocab.Vocab, bool] = True, *, max_length: int = 1000000, meta: Dict[str, Any] = {}, create_tokenizer: Union[Callable[[ForwardRef('Language')], Callable[[str], spacy.tokens.doc.Doc]], NoneType] = None, batch_size: int = 1000, **kwargs) -> None\n",
      " |      Initialise a Language object.\n",
      " |      \n",
      " |      vocab (Vocab): A `Vocab` object. If `True`, a vocab is created.\n",
      " |      meta (dict): Custom meta data for the Language class. Is written to by\n",
      " |          models to add model meta data.\n",
      " |      max_length (int): Maximum number of characters in a single text. The\n",
      " |          current models may run out memory on extremely long texts, due to\n",
      " |          large internal allocations. You should segment these texts into\n",
      " |          meaningful units, e.g. paragraphs, subsections etc, before passing\n",
      " |          them to spaCy. Default maximum length is 1,000,000 charas (1mb). As\n",
      " |          a rule of thumb, if all pipeline components are enabled, spaCy's\n",
      " |          default models currently requires roughly 1GB of temporary memory per\n",
      " |          100,000 characters in one text.\n",
      " |      create_tokenizer (Callable): Function that takes the nlp object and\n",
      " |          returns a tokenizer.\n",
      " |      batch_size (int): Default batch size for pipe and evaluate.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#init\n",
      " |  \n",
      " |  add_pipe(self, factory_name: str, name: Union[str, NoneType] = None, *, before: Union[str, int, NoneType] = None, after: Union[str, int, NoneType] = None, first: Union[bool, NoneType] = None, last: Union[bool, NoneType] = None, source: Union[ForwardRef('Language'), NoneType] = None, config: Dict[str, Any] = {}, raw_config: Union[thinc.config.Config, NoneType] = None, validate: bool = True) -> 'Pipe'\n",
      " |      Add a component to the processing pipeline. Valid components are\n",
      " |      callables that take a `Doc` object, modify it and return it. Only one\n",
      " |      of before/after/first/last can be set. Default behaviour is \"last\".\n",
      " |      \n",
      " |      factory_name (str): Name of the component factory.\n",
      " |      name (str): Name of pipeline component. Overwrites existing\n",
      " |          component.name attribute if available. If no name is set and\n",
      " |          the component exposes no name attribute, component.__name__ is\n",
      " |          used. An error is raised if a name already exists in the pipeline.\n",
      " |      before (Union[str, int]): Name or index of the component to insert new\n",
      " |          component directly before.\n",
      " |      after (Union[str, int]): Name or index of the component to insert new\n",
      " |          component directly after.\n",
      " |      first (bool): If True, insert component first in the pipeline.\n",
      " |      last (bool): If True, insert component last in the pipeline.\n",
      " |      source (Language): Optional loaded nlp object to copy the pipeline\n",
      " |          component from.\n",
      " |      config (Dict[str, Any]): Config parameters to use for this component.\n",
      " |          Will be merged with default config, if available.\n",
      " |      raw_config (Optional[Config]): Internals: the non-interpolated config.\n",
      " |      validate (bool): Whether to validate the component config against the\n",
      " |          arguments and types expected by the factory.\n",
      " |      RETURNS (Pipe): The pipeline component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#add_pipe\n",
      " |  \n",
      " |  analyze_pipes(self, *, keys: List[str] = ['assigns', 'requires', 'scores', 'retokenizes'], pretty: bool = False) -> Union[Dict[str, Any], NoneType]\n",
      " |      Analyze the current pipeline components, print a summary of what\n",
      " |      they assign or require and check that all requirements are met.\n",
      " |      \n",
      " |      keys (List[str]): The meta values to display in the table. Corresponds\n",
      " |          to values in FactoryMeta, defined by @Language.factory decorator.\n",
      " |      pretty (bool): Pretty-print the results.\n",
      " |      RETURNS (dict): The data.\n",
      " |  \n",
      " |  begin_training(self, get_examples: Union[Callable[[], Iterable[spacy.training.example.Example]], NoneType] = None, *, sgd: Union[thinc.optimizers.Optimizer, NoneType] = None) -> thinc.optimizers.Optimizer\n",
      " |  \n",
      " |  create_optimizer(self)\n",
      " |      Create an optimizer, usually using the [training.optimizer] config.\n",
      " |  \n",
      " |  create_pipe(self, factory_name: str, name: Union[str, NoneType] = None, *, config: Dict[str, Any] = {}, raw_config: Union[thinc.config.Config, NoneType] = None, validate: bool = True) -> 'Pipe'\n",
      " |      Create a pipeline component. Mostly used internally. To create and\n",
      " |      add a component to the pipeline, you can use nlp.add_pipe.\n",
      " |      \n",
      " |      factory_name (str): Name of component factory.\n",
      " |      name (Optional[str]): Optional name to assign to component instance.\n",
      " |          Defaults to factory name if not set.\n",
      " |      config (Dict[str, Any]): Config parameters to use for this component.\n",
      " |          Will be merged with default config, if available.\n",
      " |      raw_config (Optional[Config]): Internals: the non-interpolated config.\n",
      " |      validate (bool): Whether to validate the component config against the\n",
      " |          arguments and types expected by the factory.\n",
      " |      RETURNS (Pipe): The pipeline component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#create_pipe\n",
      " |  \n",
      " |  create_pipe_from_source(self, source_name: str, source: 'Language', *, name: str) -> Tuple[ForwardRef('Pipe'), str]\n",
      " |      Create a pipeline component by copying it from an existing model.\n",
      " |      \n",
      " |      source_name (str): Name of the component in the source pipeline.\n",
      " |      source (Language): The source nlp object to copy from.\n",
      " |      name (str): Optional alternative name to use in current pipeline.\n",
      " |      RETURNS (Tuple[Callable, str]): The component and its factory name.\n",
      " |  \n",
      " |  disable_pipe(self, name: str) -> None\n",
      " |      Disable a pipeline component. The component will still exist on\n",
      " |      the nlp object, but it won't be run as part of the pipeline. Does\n",
      " |      nothing if the component is already disabled.\n",
      " |      \n",
      " |      name (str): The name of the component to disable.\n",
      " |  \n",
      " |  disable_pipes(self, *names) -> 'DisabledPipes'\n",
      " |      Disable one or more pipeline components. If used as a context\n",
      " |      manager, the pipeline will be restored to the initial state at the end\n",
      " |      of the block. Otherwise, a DisabledPipes object is returned, that has\n",
      " |      a `.restore()` method you can use to undo your changes.\n",
      " |      \n",
      " |      This method has been deprecated since 3.0\n",
      " |  \n",
      " |  enable_pipe(self, name: str) -> None\n",
      " |      Enable a previously disabled pipeline component so it's run as part\n",
      " |      of the pipeline. Does nothing if the component is already enabled.\n",
      " |      \n",
      " |      name (str): The name of the component to enable.\n",
      " |  \n",
      " |  evaluate(self, examples: Iterable[spacy.training.example.Example], *, batch_size: Union[int, NoneType] = None, scorer: Union[spacy.scorer.Scorer, NoneType] = None, component_cfg: Union[Dict[str, Dict[str, Any]], NoneType] = None, scorer_cfg: Union[Dict[str, Any], NoneType] = None) -> Dict[str, Any]\n",
      " |      Evaluate a model's pipeline components.\n",
      " |      \n",
      " |      examples (Iterable[Example]): `Example` objects.\n",
      " |      batch_size (Optional[int]): Batch size to use.\n",
      " |      scorer (Optional[Scorer]): Scorer to use. If not passed in, a new one\n",
      " |          will be created.\n",
      " |      component_cfg (dict): An optional dictionary with extra keyword\n",
      " |          arguments for specific components.\n",
      " |      scorer_cfg (dict): An optional dictionary with extra keyword arguments\n",
      " |          for the scorer.\n",
      " |      \n",
      " |      RETURNS (Scorer): The scorer containing the evaluation results.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#evaluate\n",
      " |  \n",
      " |  from_bytes(self, bytes_data: bytes, *, exclude: Iterable[str] = []) -> 'Language'\n",
      " |      Load state from a binary string.\n",
      " |      \n",
      " |      bytes_data (bytes): The data to load from.\n",
      " |      exclude (Iterable[str]): Names of components or serialization fields to exclude.\n",
      " |      RETURNS (Language): The `Language` object.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#from_bytes\n",
      " |  \n",
      " |  from_disk(self, path: Union[str, pathlib.Path], *, exclude: Iterable[str] = [], overrides: Dict[str, Any] = {}) -> 'Language'\n",
      " |      Loads state from a directory. Modifies the object in place and\n",
      " |      returns it. If the saved `Language` object contains a model, the\n",
      " |      model will be loaded.\n",
      " |      \n",
      " |      path (str / Path): A path to a directory.\n",
      " |      exclude (Iterable[str]): Names of components or serialization fields to exclude.\n",
      " |      RETURNS (Language): The modified `Language` object.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#from_disk\n",
      " |  \n",
      " |  get_pipe(self, name: str) -> 'Pipe'\n",
      " |      Get a pipeline component for a given component name.\n",
      " |      \n",
      " |      name (str): Name of pipeline component to get.\n",
      " |      RETURNS (callable): The pipeline component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#get_pipe\n",
      " |  \n",
      " |  get_pipe_config(self, name: str) -> thinc.config.Config\n",
      " |      Get the config used to create a pipeline component.\n",
      " |      \n",
      " |      name (str): The component name.\n",
      " |      RETURNS (Config): The config used to create the pipeline component.\n",
      " |  \n",
      " |  get_pipe_meta(self, name: str) -> 'FactoryMeta'\n",
      " |      Get the meta information for a given component name.\n",
      " |      \n",
      " |      name (str): The component name.\n",
      " |      RETURNS (FactoryMeta): The meta for the given component name.\n",
      " |  \n",
      " |  has_pipe(self, name: str) -> bool\n",
      " |      Check if a component name is present in the pipeline. Equivalent to\n",
      " |      `name in nlp.pipe_names`.\n",
      " |      \n",
      " |      name (str): Name of the component.\n",
      " |      RETURNS (bool): Whether a component of the name exists in the pipeline.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#has_pipe\n",
      " |  \n",
      " |  initialize(self, get_examples: Union[Callable[[], Iterable[spacy.training.example.Example]], NoneType] = None, *, sgd: Union[thinc.optimizers.Optimizer, NoneType] = None) -> thinc.optimizers.Optimizer\n",
      " |      Initialize the pipe for training, using data examples if available.\n",
      " |      \n",
      " |      get_examples (Callable[[], Iterable[Example]]): Optional function that\n",
      " |          returns gold-standard Example objects.\n",
      " |      sgd (Optional[Optimizer]): An optimizer to use for updates. If not\n",
      " |          provided, will be created using the .create_optimizer() method.\n",
      " |      RETURNS (thinc.api.Optimizer): The optimizer.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#initialize\n",
      " |  \n",
      " |  make_doc(self, text: str) -> spacy.tokens.doc.Doc\n",
      " |      Turn a text into a Doc object.\n",
      " |      \n",
      " |      text (str): The text to process.\n",
      " |      RETURNS (Doc): The processed doc.\n",
      " |  \n",
      " |  pipe(self, texts: Union[Iterable[Union[str, spacy.tokens.doc.Doc]], Iterable[Tuple[Union[str, spacy.tokens.doc.Doc], ~_AnyContext]]], *, as_tuples: bool = False, batch_size: Union[int, NoneType] = None, disable: Iterable[str] = [], component_cfg: Union[Dict[str, Dict[str, Any]], NoneType] = None, n_process: int = 1) -> Union[Iterator[spacy.tokens.doc.Doc], Iterator[Tuple[spacy.tokens.doc.Doc, ~_AnyContext]]]\n",
      " |      Process texts as a stream, and yield `Doc` objects in order.\n",
      " |      \n",
      " |      texts (Iterable[Union[str, Doc]]): A sequence of texts or docs to\n",
      " |          process.\n",
      " |      as_tuples (bool): If set to True, inputs should be a sequence of\n",
      " |          (text, context) tuples. Output will then be a sequence of\n",
      " |          (doc, context) tuples. Defaults to False.\n",
      " |      batch_size (Optional[int]): The number of texts to buffer.\n",
      " |      disable (List[str]): Names of the pipeline components to disable.\n",
      " |      component_cfg (Dict[str, Dict]): An optional dictionary with extra keyword\n",
      " |          arguments for specific components.\n",
      " |      n_process (int): Number of processors to process texts. If -1, set `multiprocessing.cpu_count()`.\n",
      " |      YIELDS (Doc): Documents in the order of the original text.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#pipe\n",
      " |  \n",
      " |  rehearse(self, examples: Iterable[spacy.training.example.Example], *, sgd: Union[thinc.optimizers.Optimizer, NoneType] = None, losses: Union[Dict[str, float], NoneType] = None, component_cfg: Union[Dict[str, Dict[str, Any]], NoneType] = None, exclude: Iterable[str] = []) -> Dict[str, float]\n",
      " |      Make a \"rehearsal\" update to the models in the pipeline, to prevent\n",
      " |      forgetting. Rehearsal updates run an initial copy of the model over some\n",
      " |      data, and update the model so its current predictions are more like the\n",
      " |      initial ones. This is useful for keeping a pretrained model on-track,\n",
      " |      even if you're updating it with a smaller set of examples.\n",
      " |      \n",
      " |      examples (Iterable[Example]): A batch of `Example` objects.\n",
      " |      sgd (Optional[Optimizer]): An optimizer.\n",
      " |      component_cfg (Dict[str, Dict]): Config parameters for specific pipeline\n",
      " |          components, keyed by component name.\n",
      " |      exclude (Iterable[str]): Names of components that shouldn't be updated.\n",
      " |      RETURNS (dict): Results from the update.\n",
      " |      \n",
      " |      EXAMPLE:\n",
      " |          >>> raw_text_batches = minibatch(raw_texts)\n",
      " |          >>> for labelled_batch in minibatch(examples):\n",
      " |          >>>     nlp.update(labelled_batch)\n",
      " |          >>>     raw_batch = [Example.from_dict(nlp.make_doc(text), {}) for text in next(raw_text_batches)]\n",
      " |          >>>     nlp.rehearse(raw_batch)\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#rehearse\n",
      " |  \n",
      " |  remove_pipe(self, name: str) -> Tuple[str, ForwardRef('Pipe')]\n",
      " |      Remove a component from the pipeline.\n",
      " |      \n",
      " |      name (str): Name of the component to remove.\n",
      " |      RETURNS (tuple): A `(name, component)` tuple of the removed component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#remove_pipe\n",
      " |  \n",
      " |  rename_pipe(self, old_name: str, new_name: str) -> None\n",
      " |      Rename a pipeline component.\n",
      " |      \n",
      " |      old_name (str): Name of the component to rename.\n",
      " |      new_name (str): New name of the component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#rename_pipe\n",
      " |  \n",
      " |  replace_listeners(self, tok2vec_name: str, pipe_name: str, listeners: Iterable[str]) -> None\n",
      " |      Find listener layers (connecting to a token-to-vector embedding\n",
      " |      component) of a given pipeline component model and replace\n",
      " |      them with a standalone copy of the token-to-vector layer. This can be\n",
      " |      useful when training a pipeline with components sourced from an existing\n",
      " |      pipeline: if multiple components (e.g. tagger, parser, NER) listen to\n",
      " |      the same tok2vec component, but some of them are frozen and not updated,\n",
      " |      their performance may degrade significally as the tok2vec component is\n",
      " |      updated with new data. To prevent this, listeners can be replaced with\n",
      " |      a standalone tok2vec layer that is owned by the component and doesn't\n",
      " |      change if the component isn't updated.\n",
      " |      \n",
      " |      tok2vec_name (str): Name of the token-to-vector component, typically\n",
      " |          \"tok2vec\" or \"transformer\".\n",
      " |      pipe_name (str): Name of pipeline component to replace listeners for.\n",
      " |      listeners (Iterable[str]): The paths to the listeners, relative to the\n",
      " |          component config, e.g. [\"model.tok2vec\"]. Typically, implementations\n",
      " |          will only connect to one tok2vec component, [model.tok2vec], but in\n",
      " |          theory, custom models can use multiple listeners. The value here can\n",
      " |          either be an empty list to not replace any listeners, or a complete\n",
      " |          (!) list of the paths to all listener layers used by the model.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#replace_listeners\n",
      " |  \n",
      " |  replace_pipe(self, name: str, factory_name: str, *, config: Dict[str, Any] = {}, validate: bool = True) -> 'Pipe'\n",
      " |      Replace a component in the pipeline.\n",
      " |      \n",
      " |      name (str): Name of the component to replace.\n",
      " |      factory_name (str): Factory name of replacement component.\n",
      " |      config (Optional[Dict[str, Any]]): Config parameters to use for this\n",
      " |          component. Will be merged with default config, if available.\n",
      " |      validate (bool): Whether to validate the component config against the\n",
      " |          arguments and types expected by the factory.\n",
      " |      RETURNS (Pipe): The new pipeline component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#replace_pipe\n",
      " |  \n",
      " |  resume_training(self, *, sgd: Union[thinc.optimizers.Optimizer, NoneType] = None) -> thinc.optimizers.Optimizer\n",
      " |      Continue training a pretrained model.\n",
      " |      \n",
      " |      Create and return an optimizer, and initialize \"rehearsal\" for any pipeline\n",
      " |      component that has a .rehearse() method. Rehearsal is used to prevent\n",
      " |      models from \"forgetting\" their initialized \"knowledge\". To perform\n",
      " |      rehearsal, collect samples of text you want the models to retain performance\n",
      " |      on, and call nlp.rehearse() with a batch of Example objects.\n",
      " |      \n",
      " |      RETURNS (Optimizer): The optimizer.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#resume_training\n",
      " |  \n",
      " |  select_pipes(self, *, disable: Union[str, Iterable[str], NoneType] = None, enable: Union[str, Iterable[str], NoneType] = None) -> 'DisabledPipes'\n",
      " |      Disable one or more pipeline components. If used as a context\n",
      " |      manager, the pipeline will be restored to the initial state at the end\n",
      " |      of the block. Otherwise, a DisabledPipes object is returned, that has\n",
      " |      a `.restore()` method you can use to undo your changes.\n",
      " |      \n",
      " |      disable (str or iterable): The name(s) of the pipes to disable\n",
      " |      enable (str or iterable): The name(s) of the pipes to enable - all others will be disabled\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#select_pipes\n",
      " |  \n",
      " |  set_error_handler(self, error_handler: Callable[[str, ForwardRef('Pipe'), List[spacy.tokens.doc.Doc], Exception], NoReturn])\n",
      " |      Set an error handler object for all the components in the pipeline that implement\n",
      " |      a set_error_handler function.\n",
      " |      \n",
      " |      error_handler (Callable[[str, Pipe, List[Doc], Exception], NoReturn]):\n",
      " |          Function that deals with a failing batch of documents. This callable function should take in\n",
      " |          the component's name, the component itself, the offending batch of documents, and the exception\n",
      " |          that was thrown.\n",
      " |      DOCS: https://spacy.io/api/language#set_error_handler\n",
      " |  \n",
      " |  to_bytes(self, *, exclude: Iterable[str] = []) -> bytes\n",
      " |      Serialize the current state to a binary string.\n",
      " |      \n",
      " |      exclude (Iterable[str]): Names of components or serialization fields to exclude.\n",
      " |      RETURNS (bytes): The serialized form of the `Language` object.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#to_bytes\n",
      " |  \n",
      " |  to_disk(self, path: Union[str, pathlib.Path], *, exclude: Iterable[str] = []) -> None\n",
      " |      Save the current state to a directory.  If a model is loaded, this\n",
      " |      will include the model.\n",
      " |      \n",
      " |      path (str / Path): Path to a directory, which will be created if\n",
      " |          it doesn't exist.\n",
      " |      exclude (Iterable[str]): Names of components or serialization fields to exclude.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#to_disk\n",
      " |  \n",
      " |  update(self, examples: Iterable[spacy.training.example.Example], _: Union[Any, NoneType] = None, *, drop: float = 0.0, sgd: Union[thinc.optimizers.Optimizer, NoneType] = None, losses: Union[Dict[str, float], NoneType] = None, component_cfg: Union[Dict[str, Dict[str, Any]], NoneType] = None, exclude: Iterable[str] = [], annotates: Iterable[str] = [])\n",
      " |      Update the models in the pipeline.\n",
      " |      \n",
      " |      examples (Iterable[Example]): A batch of examples\n",
      " |      _: Should not be set - serves to catch backwards-incompatible scripts.\n",
      " |      drop (float): The dropout rate.\n",
      " |      sgd (Optimizer): An optimizer.\n",
      " |      losses (Dict[str, float]): Dictionary to update with the loss, keyed by\n",
      " |          component.\n",
      " |      component_cfg (Dict[str, Dict]): Config parameters for specific pipeline\n",
      " |          components, keyed by component name.\n",
      " |      exclude (Iterable[str]): Names of components that shouldn't be updated.\n",
      " |      annotates (Iterable[str]): Names of components that should set\n",
      " |          annotations on the predicted examples after updating.\n",
      " |      RETURNS (Dict[str, float]): The updated losses dictionary\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#update\n",
      " |  \n",
      " |  use_params(self, params: Union[dict, NoneType])\n",
      " |      Replace weights of models in the pipeline with those provided in the\n",
      " |      params dictionary. Can be used as a contextmanager, in which case,\n",
      " |      models go back to their original weights after the block.\n",
      " |      \n",
      " |      params (dict): A dictionary of parameters keyed by model ID.\n",
      " |      \n",
      " |      EXAMPLE:\n",
      " |          >>> with nlp.use_params(optimizer.averages):\n",
      " |          >>>     nlp.to_disk(\"/tmp/checkpoint\")\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#use_params\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from spacy.language.Language:\n",
      " |  \n",
      " |  __init_subclass__(**kwargs) from builtins.type\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      " |  \n",
      " |  component(name: str, *, assigns: Iterable[str] = [], requires: Iterable[str] = [], retokenizes: bool = False, func: Union[ForwardRef('Pipe'), NoneType] = None) -> Callable from builtins.type\n",
      " |      Register a new pipeline component. Can be used for stateless function\n",
      " |      components that don't require a separate factory. Can be used as a\n",
      " |      decorator on a function or classmethod, or called as a function with the\n",
      " |      factory provided as the func keyword argument. To create a component and\n",
      " |      add it to the pipeline, you can use nlp.add_pipe(name).\n",
      " |      \n",
      " |      name (str): The name of the component factory.\n",
      " |      assigns (Iterable[str]): Doc/Token attributes assigned by this component,\n",
      " |          e.g. \"token.ent_id\". Used for pipeline analysis.\n",
      " |      requires (Iterable[str]): Doc/Token attributes required by this component,\n",
      " |          e.g. \"token.ent_id\". Used for pipeline analysis.\n",
      " |      retokenizes (bool): Whether the component changes the tokenization.\n",
      " |          Used for pipeline analysis.\n",
      " |      func (Optional[Callable]): Factory function if not used as a decorator.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#component\n",
      " |  \n",
      " |  factory(name: str, *, default_config: Dict[str, Any] = {}, assigns: Iterable[str] = [], requires: Iterable[str] = [], retokenizes: bool = False, default_score_weights: Dict[str, Union[float, NoneType]] = {}, func: Union[Callable, NoneType] = None) -> Callable from builtins.type\n",
      " |      Register a new pipeline component factory. Can be used as a decorator\n",
      " |      on a function or classmethod, or called as a function with the factory\n",
      " |      provided as the func keyword argument. To create a component and add\n",
      " |      it to the pipeline, you can use nlp.add_pipe(name).\n",
      " |      \n",
      " |      name (str): The name of the component factory.\n",
      " |      default_config (Dict[str, Any]): Default configuration, describing the\n",
      " |          default values of the factory arguments.\n",
      " |      assigns (Iterable[str]): Doc/Token attributes assigned by this component,\n",
      " |          e.g. \"token.ent_id\". Used for pipeline analysis.\n",
      " |      requires (Iterable[str]): Doc/Token attributes required by this component,\n",
      " |          e.g. \"token.ent_id\". Used for pipeline analysis.\n",
      " |      retokenizes (bool): Whether the component changes the tokenization.\n",
      " |          Used for pipeline analysis.\n",
      " |      default_score_weights (Dict[str, Optional[float]]): The scores to report during\n",
      " |          training, and their default weight towards the final score used to\n",
      " |          select the best model. Weights should sum to 1.0 per component and\n",
      " |          will be combined and normalized for the whole pipeline. If None,\n",
      " |          the score won't be shown in the logs or be weighted.\n",
      " |      func (Optional[Callable]): Factory function if not used as a decorator.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#factory\n",
      " |  \n",
      " |  from_config(config: Union[Dict[str, Any], thinc.config.Config] = {}, *, vocab: Union[spacy.vocab.Vocab, bool] = True, disable: Iterable[str] = [], exclude: Iterable[str] = [], meta: Dict[str, Any] = {}, auto_fill: bool = True, validate: bool = True) -> 'Language' from builtins.type\n",
      " |      Create the nlp object from a loaded config. Will set up the tokenizer\n",
      " |      and language data, add pipeline components etc. If no config is provided,\n",
      " |      the default config of the given language is used.\n",
      " |      \n",
      " |      config (Dict[str, Any] / Config): The loaded config.\n",
      " |      vocab (Vocab): A Vocab object. If True, a vocab is created.\n",
      " |      disable (Iterable[str]): Names of pipeline components to disable.\n",
      " |          Disabled pipes will be loaded but they won't be run unless you\n",
      " |          explicitly enable them by calling nlp.enable_pipe.\n",
      " |      exclude (Iterable[str]): Names of pipeline components to exclude.\n",
      " |          Excluded components won't be loaded.\n",
      " |      meta (Dict[str, Any]): Meta overrides for nlp.meta.\n",
      " |      auto_fill (bool): Automatically fill in missing values in config based\n",
      " |          on defaults and function argument annotations.\n",
      " |      validate (bool): Validate the component config and arguments against\n",
      " |          the types expected by the factory.\n",
      " |      RETURNS (Language): The initialized Language class.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#from_config\n",
      " |  \n",
      " |  get_factory_meta(name: str) -> 'FactoryMeta' from builtins.type\n",
      " |      Get the meta information for a given factory name.\n",
      " |      \n",
      " |      name (str): The component factory name.\n",
      " |      RETURNS (FactoryMeta): The meta for the given factory name.\n",
      " |  \n",
      " |  get_factory_name(name: str) -> str from builtins.type\n",
      " |      Get the internal factory name based on the language subclass.\n",
      " |      \n",
      " |      name (str): The factory name.\n",
      " |      RETURNS (str): The internal factory name.\n",
      " |  \n",
      " |  has_factory(name: str) -> bool from builtins.type\n",
      " |      RETURNS (bool): Whether a factory of that name is registered.\n",
      " |  \n",
      " |  set_factory_meta(name: str, value: 'FactoryMeta') -> None from builtins.type\n",
      " |      Set the meta information for a given factory name.\n",
      " |      \n",
      " |      name (str): The component factory name.\n",
      " |      value (FactoryMeta): The meta to set.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from spacy.language.Language:\n",
      " |  \n",
      " |  component_names\n",
      " |      Get the names of the available pipeline components. Includes all\n",
      " |      active and inactive pipeline components.\n",
      " |      \n",
      " |      RETURNS (List[str]): List of component name strings, in order.\n",
      " |  \n",
      " |  components\n",
      " |      Get all (name, component) tuples in the pipeline, including the\n",
      " |      currently disabled components.\n",
      " |  \n",
      " |  disabled\n",
      " |      Get the names of all disabled components.\n",
      " |      \n",
      " |      RETURNS (List[str]): The disabled components.\n",
      " |  \n",
      " |  factory_names\n",
      " |      Get names of all available factories.\n",
      " |      \n",
      " |      RETURNS (List[str]): The factory names.\n",
      " |  \n",
      " |  path\n",
      " |  \n",
      " |  pipe_factories\n",
      " |      Get the component factories for the available pipeline components.\n",
      " |      \n",
      " |      RETURNS (Dict[str, str]): Factory names, keyed by component names.\n",
      " |  \n",
      " |  pipe_labels\n",
      " |      Get the labels set by the pipeline components, if available (if\n",
      " |      the component exposes a labels property).\n",
      " |      \n",
      " |      RETURNS (Dict[str, List[str]]): Labels keyed by component name.\n",
      " |  \n",
      " |  pipe_names\n",
      " |      Get names of available active pipeline components.\n",
      " |      \n",
      " |      RETURNS (List[str]): List of component name strings, in order.\n",
      " |  \n",
      " |  pipeline\n",
      " |      The processing pipeline consisting of (name, component) tuples. The\n",
      " |      components are called on the Doc in order as it passes through the\n",
      " |      pipeline.\n",
      " |      \n",
      " |      RETURNS (List[Tuple[str, Pipe]]): The pipeline.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from spacy.language.Language:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  config\n",
      " |      Trainable config for the current language instance. Includes the\n",
      " |      current pipeline components, as well as default training config.\n",
      " |      \n",
      " |      RETURNS (thinc.api.Config): The config.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#config\n",
      " |  \n",
      " |  meta\n",
      " |      Custom meta data of the language class. If a model is loaded, this\n",
      " |      includes details from the model's meta.json.\n",
      " |      \n",
      " |      RETURNS (Dict[str, Any]): The meta.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#meta\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from spacy.language.Language:\n",
      " |  \n",
      " |  __annotations__ = {'_factory_meta': typing.Dict[str, ForwardRef('Facto...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54b5c4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Apple is looking for buying a U.K. startup for $1 billion'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6eb8f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f943915a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c36e356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Doc object:\n",
      "\n",
      "class Doc(builtins.object)\n",
      " |  Doc(Vocab vocab, words=None, spaces=None, user_data=None, *, tags=None, pos=None, morphs=None, lemmas=None, heads=None, deps=None, sent_starts=None, ents=None)\n",
      " |  A sequence of Token objects. Access sentences and named entities, export\n",
      " |      annotations to numpy arrays, losslessly serialize to compressed binary\n",
      " |      strings. The `Doc` object holds an array of `TokenC` structs. The\n",
      " |      Python-level `Token` and `Span` objects are views of this array, i.e.\n",
      " |      they don't own the data themselves.\n",
      " |  \n",
      " |      EXAMPLE:\n",
      " |          Construction 1\n",
      " |          >>> doc = nlp(u'Some text')\n",
      " |  \n",
      " |          Construction 2\n",
      " |          >>> from spacy.tokens import Doc\n",
      " |          >>> doc = Doc(nlp.vocab, words=[\"hello\", \"world\", \"!\"], spaces=[True, False, False])\n",
      " |  \n",
      " |      DOCS: https://spacy.io/api/doc\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __bytes__(...)\n",
      " |      Doc.__bytes__(self)\n",
      " |  \n",
      " |  __getitem__(...)\n",
      " |      Get a `Token` or `Span` object.\n",
      " |      \n",
      " |      i (int or tuple) The index of the token, or the slice of the document\n",
      " |          to get.\n",
      " |      RETURNS (Token or Span): The token at `doc[i]]`, or the span at\n",
      " |          `doc[start : end]`.\n",
      " |      \n",
      " |      EXAMPLE:\n",
      " |          >>> doc[i]\n",
      " |          Get the `Token` object at position `i`, where `i` is an integer.\n",
      " |          Negative indexing is supported, and follows the usual Python\n",
      " |          semantics, i.e. `doc[-2]` is `doc[len(doc) - 2]`.\n",
      " |      \n",
      " |          >>> doc[start : end]]\n",
      " |          Get a `Span` object, starting at position `start` and ending at\n",
      " |          position `end`, where `start` and `end` are token indices. For\n",
      " |          instance, `doc[2:5]` produces a span consisting of tokens 2, 3 and\n",
      " |          4. Stepped slices (e.g. `doc[start : end : step]`) are not\n",
      " |          supported, as `Span` objects must be contiguous (cannot have gaps).\n",
      " |          You can use negative indices and open-ended ranges, which have\n",
      " |          their normal Python semantics.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/doc#getitem\n",
      " |  \n",
      " |  __init__(...)\n",
      " |      Create a Doc object.\n",
      " |      \n",
      " |      vocab (Vocab): A vocabulary object, which must match any models you\n",
      " |          want to use (e.g. tokenizer, parser, entity recognizer).\n",
      " |      words (Optional[List[Union[str, int]]]): A list of unicode strings or\n",
      " |          hash values to add to the document as words. If `None`, defaults to\n",
      " |          empty list.\n",
      " |      spaces (Optional[List[bool]]): A list of boolean values, of the same\n",
      " |          length as `words`. `True` means that the word is followed by a space,\n",
      " |          `False` means it is not. If `None`, defaults to `[True]*len(words)`\n",
      " |      user_data (dict or None): Optional extra data to attach to the Doc.\n",
      " |      tags (Optional[List[str]]): A list of unicode strings, of the same\n",
      " |          length as words, to assign as token.tag. Defaults to None.\n",
      " |      pos (Optional[List[str]]): A list of unicode strings, of the same\n",
      " |          length as words, to assign as token.pos. Defaults to None.\n",
      " |      morphs (Optional[List[str]]): A list of unicode strings, of the same\n",
      " |          length as words, to assign as token.morph. Defaults to None.\n",
      " |      lemmas (Optional[List[str]]): A list of unicode strings, of the same\n",
      " |          length as words, to assign as token.lemma. Defaults to None.\n",
      " |      heads (Optional[List[int]]): A list of values, of the same length as\n",
      " |          words, to assign as heads. Head indices are the position of the\n",
      " |          head in the doc. Defaults to None.\n",
      " |      deps (Optional[List[str]]): A list of unicode strings, of the same\n",
      " |          length as words, to assign as token.dep. Defaults to None.\n",
      " |      sent_starts (Optional[List[Union[bool, None]]]): A list of values, of\n",
      " |          the same length as words, to assign as token.is_sent_start. Will be\n",
      " |          overridden by heads if heads is provided. Defaults to None.\n",
      " |      ents (Optional[List[str]]): A list of unicode strings, of the same\n",
      " |          length as words, as IOB tags to assign as token.ent_iob and\n",
      " |          token.ent_type. Defaults to None.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/doc#init\n",
      " |  \n",
      " |  __iter__(...)\n",
      " |      Iterate over `Token`  objects, from which the annotations can be\n",
      " |      easily accessed. This is the main way of accessing `Token` objects,\n",
      " |      which are the main way annotations are accessed from Python. If faster-\n",
      " |      than-Python speeds are required, you can instead access the annotations\n",
      " |      as a numpy array, or access the underlying C data directly from Cython.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/doc#iter\n",
      " |  \n",
      " |  __len__(...)\n",
      " |      The number of tokens in the document.\n",
      " |      \n",
      " |      RETURNS (int): The number of tokens in the document.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/doc#len\n",
      " |  \n",
      " |  __reduce__ = __reduce_cython__(...)\n",
      " |      Doc.__reduce_cython__(self)\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__ = __setstate_cython__(...)\n",
      " |      Doc.__setstate_cython__(self, __pyx_state)\n",
      " |  \n",
      " |  __str__(self, /)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  __unicode__(...)\n",
      " |      Doc.__unicode__(self)\n",
      " |  \n",
      " |  char_span(...)\n",
      " |      Doc.char_span(self, int start_idx, int end_idx, label=0, kb_id=0, vector=None, alignment_mode='strict')\n",
      " |      Create a `Span` object from the slice\n",
      " |              `doc.text[start_idx : end_idx]`. Returns None if no valid `Span` can be\n",
      " |              created.\n",
      " |      \n",
      " |              doc (Doc): The parent document.\n",
      " |              start_idx (int): The index of the first character of the span.\n",
      " |              end_idx (int): The index of the first character after the span.\n",
      " |              label (uint64 or string): A label to attach to the Span, e.g. for\n",
      " |                  named entities.\n",
      " |              kb_id (uint64 or string):  An ID from a KB to capture the meaning of a\n",
      " |                  named entity.\n",
      " |              vector (ndarray[ndim=1, dtype='float32']): A meaning representation of\n",
      " |                  the span.\n",
      " |              alignment_mode (str): How character indices are aligned to token\n",
      " |                  boundaries. Options: \"strict\" (character indices must be aligned\n",
      " |                  with token boundaries), \"contract\" (span of all tokens completely\n",
      " |                  within the character span), \"expand\" (span of all tokens at least\n",
      " |                  partially covered by the character span). Defaults to \"strict\".\n",
      " |              RETURNS (Span): The newly constructed object.\n",
      " |      \n",
      " |              DOCS: https://spacy.io/api/doc#char_span\n",
      " |  \n",
      " |  copy(...)\n",
      " |      Doc.copy(self)\n",
      " |  \n",
      " |  count_by(...)\n",
      " |      Doc.count_by(self, attr_id_t attr_id, exclude=None, counts=None)\n",
      " |      Count the frequencies of a given attribute. Produces a dict of\n",
      " |              `{attribute (int): count (ints)}` frequencies, keyed by the values of\n",
      " |              the given attribute ID.\n",
      " |      \n",
      " |              attr_id (int): The attribute ID to key the counts.\n",
      " |              RETURNS (dict): A dictionary mapping attributes to integer counts.\n",
      " |      \n",
      " |              DOCS: https://spacy.io/api/doc#count_by\n",
      " |  \n",
      " |  extend_tensor(...)\n",
      " |      Doc.extend_tensor(self, tensor)\n",
      " |      Concatenate a new tensor onto the doc.tensor object.\n",
      " |      \n",
      " |              The doc.tensor attribute holds dense feature vectors\n",
      " |              computed by the models in the pipeline. Let's say a\n",
      " |              document with 30 words has a tensor with 128 dimensions\n",
      " |              per word. doc.tensor.shape will be (30, 128). After\n",
      " |              calling doc.extend_tensor with an array of shape (30, 64),\n",
      " |              doc.tensor == (30, 192).\n",
      " |  \n",
      " |  from_array(...)\n",
      " |      Doc.from_array(self, attrs, array)\n",
      " |      Load attributes from a numpy array. Write to a `Doc` object, from an\n",
      " |              `(M, N)` array of attributes.\n",
      " |      \n",
      " |              attrs (list) A list of attribute ID ints.\n",
      " |              array (numpy.ndarray[ndim=2, dtype='int32']): The attribute values.\n",
      " |              RETURNS (Doc): Itself.\n",
      " |      \n",
      " |              DOCS: https://spacy.io/api/doc#from_array\n",
      " |  \n",
      " |  from_bytes(...)\n",
      " |      Doc.from_bytes(self, bytes_data, *, exclude=tuple())\n",
      " |      Deserialize, i.e. import the document contents from a binary string.\n",
      " |      \n",
      " |              data (bytes): The string to load from.\n",
      " |              exclude (list): String names of serialization fields to exclude.\n",
      " |              RETURNS (Doc): Itself.\n",
      " |      \n",
      " |              DOCS: https://spacy.io/api/doc#from_bytes\n",
      " |  \n",
      " |  from_dict(...)\n",
      " |      Doc.from_dict(self, msg, *, exclude=tuple())\n",
      " |      Deserialize, i.e. import the document contents from a binary string.\n",
      " |      \n",
      " |              data (bytes): The string to load from.\n",
      " |              exclude (list): String names of serialization fields to exclude.\n",
      " |              RETURNS (Doc): Itself.\n",
      " |      \n",
      " |              DOCS: https://spacy.io/api/doc#from_dict\n",
      " |  \n",
      " |  from_disk(...)\n",
      " |      Doc.from_disk(self, path, *, exclude=tuple())\n",
      " |      Loads state from a directory. Modifies the object in place and\n",
      " |              returns it.\n",
      " |      \n",
      " |              path (str / Path): A path to a directory. Paths may be either\n",
      " |                  strings or `Path`-like objects.\n",
      " |              exclude (list): String names of serialization fields to exclude.\n",
      " |              RETURNS (Doc): The modified `Doc` object.\n",
      " |      \n",
      " |              DOCS: https://spacy.io/api/doc#from_disk\n",
      " |  \n",
      " |  get_lca_matrix(...)\n",
      " |      Doc.get_lca_matrix(self)\n",
      " |      Calculates a matrix of Lowest Common Ancestors (LCA) for a given\n",
      " |              `Doc`, where LCA[i, j] is the index of the lowest common ancestor among\n",
      " |              token i and j.\n",
      " |      \n",
      " |              RETURNS (np.array[ndim=2, dtype=numpy.int32]): LCA matrix with shape\n",
      " |                  (n, n), where n = len(self).\n",
      " |      \n",
      " |              DOCS: https://spacy.io/api/doc#get_lca_matrix\n",
      " |  \n",
      " |  has_annotation(...)\n",
      " |      Doc.has_annotation(self, attr, *, require_complete=False)\n",
      " |      Check whether the doc contains annotation on a token attribute.\n",
      " |      \n",
      " |              attr (Union[int, str]): The attribute string name or int ID.\n",
      " |              require_complete (bool): Whether to check that the attribute is set on\n",
      " |                  every token in the doc.\n",
      " |              RETURNS (bool): Whether annotation is present.\n",
      " |      \n",
      " |              DOCS: https://spacy.io/api/doc#has_annotation\n",
      " |  \n",
      " |  retokenize(...)\n",
      " |      Doc.retokenize(self)\n",
      " |      Context manager to handle retokenization of the Doc.\n",
      " |              Modifications to the Doc's tokenization are stored, and then\n",
      " |              made all at once when the context manager exits. This is\n",
      " |              much more efficient, and less error-prone.\n",
      " |      \n",
      " |              All views of the Doc (Span and Token) created before the\n",
      " |              retokenization are invalidated, although they may accidentally\n",
      " |              continue to work.\n",
      " |      \n",
      " |              DOCS: https://spacy.io/api/doc#retokenize\n",
      " |              USAGE: https://spacy.io/usage/linguistic-features#retokenization\n",
      " |  \n",
      " |  set_ents(...)\n",
      " |      Doc.set_ents(self, entities, *, blocked=None, missing=None, outside=None, default=SetEntsDefault.outside)\n",
      " |      Set entity annotation.\n",
      " |      \n",
      " |              entities (List[Span]): Spans with labels to set as entities.\n",
      " |              blocked (Optional[List[Span]]): Spans to set as 'blocked' (never an\n",
      " |                  entity) for spacy's built-in NER component. Other components may\n",
      " |                  ignore this setting.\n",
      " |              missing (Optional[List[Span]]): Spans with missing/unknown entity\n",
      " |                  information.\n",
      " |              outside (Optional[List[Span]]): Spans outside of entities (O in IOB).\n",
      " |              default (str): How to set entity annotation for tokens outside of any\n",
      " |                  provided spans. Options: \"blocked\", \"missing\", \"outside\" and\n",
      " |                  \"unmodified\" (preserve current state). Defaults to \"outside\".\n",
      " |  \n",
      " |  similarity(...)\n",
      " |      Doc.similarity(self, other)\n",
      " |      Make a semantic similarity estimate. The default estimate is cosine\n",
      " |              similarity using an average of word vectors.\n",
      " |      \n",
      " |              other (object): The object to compare with. By default, accepts `Doc`,\n",
      " |                  `Span`, `Token` and `Lexeme` objects.\n",
      " |              RETURNS (float): A scalar similarity score. Higher is more similar.\n",
      " |      \n",
      " |              DOCS: https://spacy.io/api/doc#similarity\n",
      " |  \n",
      " |  to_array(...)\n",
      " |      Doc.to_array(self, py_attr_ids) -> ndarray\n",
      " |      Export given token attributes to a numpy `ndarray`.\n",
      " |              If `attr_ids` is a sequence of M attributes, the output array will be\n",
      " |              of shape `(N, M)`, where N is the length of the `Doc` (in tokens). If\n",
      " |              `attr_ids` is a single attribute, the output shape will be (N,). You\n",
      " |              can specify attributes by integer ID (e.g. spacy.attrs.LEMMA) or\n",
      " |              string name (e.g. 'LEMMA' or 'lemma').\n",
      " |      \n",
      " |              py_attr_ids (list[]): A list of attributes (int IDs or string names).\n",
      " |              RETURNS (numpy.ndarray[long, ndim=2]): A feature matrix, with one row\n",
      " |                  per word, and one column per attribute indicated in the input\n",
      " |                  `attr_ids`.\n",
      " |      \n",
      " |              EXAMPLE:\n",
      " |                  >>> from spacy.attrs import LOWER, POS, ENT_TYPE, IS_ALPHA\n",
      " |                  >>> doc = nlp(text)\n",
      " |                  >>> # All strings mapped to integers, for easy export to numpy\n",
      " |                  >>> np_array = doc.to_array([LOWER, POS, ENT_TYPE, IS_ALPHA])\n",
      " |  \n",
      " |  to_bytes(...)\n",
      " |      Doc.to_bytes(self, *, exclude=tuple())\n",
      " |      Serialize, i.e. export the document contents to a binary string.\n",
      " |      \n",
      " |              exclude (list): String names of serialization fields to exclude.\n",
      " |              RETURNS (bytes): A losslessly serialized copy of the `Doc`, including\n",
      " |                  all annotations.\n",
      " |      \n",
      " |              DOCS: https://spacy.io/api/doc#to_bytes\n",
      " |  \n",
      " |  to_dict(...)\n",
      " |      Doc.to_dict(self, *, exclude=tuple())\n",
      " |      Export the document contents to a dictionary for serialization.\n",
      " |      \n",
      " |              exclude (list): String names of serialization fields to exclude.\n",
      " |              RETURNS (bytes): A losslessly serialized copy of the `Doc`, including\n",
      " |                  all annotations.\n",
      " |      \n",
      " |              DOCS: https://spacy.io/api/doc#to_bytes\n",
      " |  \n",
      " |  to_disk(...)\n",
      " |      Doc.to_disk(self, path, *, exclude=tuple())\n",
      " |      Save the current state to a directory.\n",
      " |      \n",
      " |              path (str / Path): A path to a directory, which will be created if\n",
      " |                  it doesn't exist. Paths may be either strings or Path-like objects.\n",
      " |              exclude (Iterable[str]): String names of serialization fields to exclude.\n",
      " |      \n",
      " |              DOCS: https://spacy.io/api/doc#to_disk\n",
      " |  \n",
      " |  to_json(...)\n",
      " |      Doc.to_json(self, underscore=None)\n",
      " |      Convert a Doc to JSON.\n",
      " |      \n",
      " |              underscore (list): Optional list of string names of custom doc._.\n",
      " |              attributes. Attribute values need to be JSON-serializable. Values will\n",
      " |              be added to an \"_\" key in the data, e.g. \"_\": {\"foo\": \"bar\"}.\n",
      " |              RETURNS (dict): The data in spaCy's JSON format.\n",
      " |  \n",
      " |  to_utf8_array(...)\n",
      " |      Doc.to_utf8_array(self, int nr_char=-1)\n",
      " |      Encode word strings to utf8, and export to a fixed-width array\n",
      " |              of characters. Characters are placed into the array in the order:\n",
      " |                  0, -1, 1, -2, etc\n",
      " |              For example, if the array is sliced array[:, :8], the array will\n",
      " |              contain the first 4 characters and last 4 characters of each word ---\n",
      " |              with the middle characters clipped out. The value 255 is used as a pad\n",
      " |              value.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |  \n",
      " |  get_extension(...) from builtins.type\n",
      " |      Doc.get_extension(type cls, name)\n",
      " |      Look up a previously registered extension by name.\n",
      " |      \n",
      " |              name (str): Name of the extension.\n",
      " |              RETURNS (tuple): A `(default, method, getter, setter)` tuple.\n",
      " |      \n",
      " |              DOCS: https://spacy.io/api/doc#get_extension\n",
      " |  \n",
      " |  has_extension(...) from builtins.type\n",
      " |      Doc.has_extension(type cls, name)\n",
      " |      Check whether an extension has been registered.\n",
      " |      \n",
      " |              name (str): Name of the extension.\n",
      " |              RETURNS (bool): Whether the extension has been registered.\n",
      " |      \n",
      " |              DOCS: https://spacy.io/api/doc#has_extension\n",
      " |  \n",
      " |  remove_extension(...) from builtins.type\n",
      " |      Doc.remove_extension(type cls, name)\n",
      " |      Remove a previously registered extension.\n",
      " |      \n",
      " |              name (str): Name of the extension.\n",
      " |              RETURNS (tuple): A `(default, method, getter, setter)` tuple of the\n",
      " |                  removed extension.\n",
      " |      \n",
      " |              DOCS: https://spacy.io/api/doc#remove_extension\n",
      " |  \n",
      " |  set_extension(...) from builtins.type\n",
      " |      Doc.set_extension(type cls, name, **kwargs)\n",
      " |      Define a custom attribute which becomes available as `Doc._`.\n",
      " |      \n",
      " |              name (str): Name of the attribute to set.\n",
      " |              default: Optional default value of the attribute.\n",
      " |              getter (callable): Optional getter function.\n",
      " |              setter (callable): Optional setter function.\n",
      " |              method (callable): Optional method for method extension.\n",
      " |              force (bool): Force overwriting existing attribute.\n",
      " |      \n",
      " |              DOCS: https://spacy.io/api/doc#set_extension\n",
      " |              USAGE: https://spacy.io/usage/processing-pipelines#custom-components-attributes\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Static methods defined here:\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  from_docs(...)\n",
      " |      Doc.from_docs(docs, ensure_whitespace=True, attrs=None)\n",
      " |      Concatenate multiple Doc objects to form a new one. Raises an error\n",
      " |              if the `Doc` objects do not all share the same `Vocab`.\n",
      " |      \n",
      " |              docs (list): A list of Doc objects.\n",
      " |              ensure_whitespace (bool): Insert a space between two adjacent docs whenever the first doc does not end in whitespace.\n",
      " |              attrs (list): Optional list of attribute ID ints or attribute name strings.\n",
      " |              RETURNS (Doc): A doc that contains the concatenated docs, or None if no docs were given.\n",
      " |      \n",
      " |              DOCS: https://spacy.io/api/doc#from_docs\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  cats\n",
      " |      cats: object\n",
      " |  \n",
      " |  doc\n",
      " |  \n",
      " |  ents\n",
      " |      The named entities in the document. Returns a tuple of named entity\n",
      " |      `Span` objects, if the entity recognizer has been applied.\n",
      " |      \n",
      " |      RETURNS (tuple): Entities in the document, one `Span` per entity.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/doc#ents\n",
      " |  \n",
      " |  has_unknown_spaces\n",
      " |      has_unknown_spaces: 'bool'\n",
      " |  \n",
      " |  has_vector\n",
      " |      A boolean value indicating whether a word vector is associated with\n",
      " |      the object.\n",
      " |      \n",
      " |      RETURNS (bool): Whether a word vector is associated with the object.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/doc#has_vector\n",
      " |  \n",
      " |  is_nered\n",
      " |  \n",
      " |  is_parsed\n",
      " |  \n",
      " |  is_sentenced\n",
      " |  \n",
      " |  is_tagged\n",
      " |  \n",
      " |  lang\n",
      " |      RETURNS (uint64): ID of the language of the doc's vocabulary.\n",
      " |  \n",
      " |  lang_\n",
      " |      RETURNS (str): Language of the doc's vocabulary, e.g. 'en'.\n",
      " |  \n",
      " |  mem\n",
      " |  \n",
      " |  noun_chunks\n",
      " |      Iterate over the base noun phrases in the document. Yields base\n",
      " |      noun-phrase #[code Span] objects, if the language has a noun chunk iterator.\n",
      " |      Raises a NotImplementedError otherwise.\n",
      " |      \n",
      " |      A base noun phrase, or \"NP chunk\", is a noun\n",
      " |      phrase that does not permit other NPs to be nested within it – so no\n",
      " |      NP-level coordination, no prepositional phrases, and no relative\n",
      " |      clauses.\n",
      " |      \n",
      " |      YIELDS (Span): Noun chunks in the document.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/doc#noun_chunks\n",
      " |  \n",
      " |  noun_chunks_iterator\n",
      " |      noun_chunks_iterator: object\n",
      " |  \n",
      " |  sentiment\n",
      " |      sentiment: 'float'\n",
      " |  \n",
      " |  sents\n",
      " |      Iterate over the sentences in the document. Yields sentence `Span`\n",
      " |      objects. Sentence spans have no label.\n",
      " |      \n",
      " |      YIELDS (Span): Sentences in the document.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/doc#sents\n",
      " |  \n",
      " |  spans\n",
      " |  \n",
      " |  tensor\n",
      " |      tensor: object\n",
      " |  \n",
      " |  text\n",
      " |      A unicode representation of the document text.\n",
      " |      \n",
      " |      RETURNS (str): The original verbatim text of the document.\n",
      " |  \n",
      " |  text_with_ws\n",
      " |      An alias of `Doc.text`, provided for duck-type compatibility with\n",
      " |      `Span` and `Token`.\n",
      " |      \n",
      " |      RETURNS (str): The original verbatim text of the document.\n",
      " |  \n",
      " |  user_data\n",
      " |      user_data: object\n",
      " |  \n",
      " |  user_hooks\n",
      " |      user_hooks: dict\n",
      " |  \n",
      " |  user_span_hooks\n",
      " |      user_span_hooks: dict\n",
      " |  \n",
      " |  user_token_hooks\n",
      " |      user_token_hooks: dict\n",
      " |  \n",
      " |  vector\n",
      " |      A real-valued meaning representation. Defaults to an average of the\n",
      " |      token vectors.\n",
      " |      \n",
      " |      RETURNS (numpy.ndarray[ndim=1, dtype='float32']): A 1D numpy array\n",
      " |          representing the document's semantics.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/doc#vector\n",
      " |  \n",
      " |  vector_norm\n",
      " |      The L2 norm of the document's vector representation.\n",
      " |      \n",
      " |      RETURNS (float): The L2 norm of the vector representation.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/doc#vector_norm\n",
      " |  \n",
      " |  vocab\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __pyx_vtable__ = <capsule object NULL>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faebba82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple\n",
      "--------------------------\n",
      "is\n",
      "--------------------------\n",
      "looking\n",
      "--------------------------\n",
      "for\n",
      "--------------------------\n",
      "buying\n",
      "--------------------------\n",
      "a\n",
      "--------------------------\n",
      "U.K.\n",
      "--------------------------\n",
      "startup\n",
      "--------------------------\n",
      "for\n",
      "--------------------------\n",
      "$\n",
      "--------------------------\n",
      "1\n",
      "--------------------------\n",
      "billion\n",
      "--------------------------\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text)\n",
    "    print(\"--------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76e8384",
   "metadata": {},
   "source": [
    "# POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db60d40f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple            PROPN\n",
      "is               AUX\n",
      "looking          VERB\n",
      "for              ADP\n",
      "buying           VERB\n",
      "a                DET\n",
      "U.K.             PROPN\n",
      "startup          NOUN\n",
      "for              ADP\n",
      "$                SYM\n",
      "1                NUM\n",
      "billion          NUM\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(f\"{token.text:{15}}  {token.pos_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45de6574",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14439f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e945963d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Apple is looking for buying a U.K. startup for $1 billion"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef484114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"f17ef79512c248af88a6336aae5d73f5-0\" class=\"displacy\" width=\"2150\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Apple</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">looking</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">buying</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">U.K.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">startup</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1625\">$</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1625\">SYM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1800\">1</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1800\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1975\">billion</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1975\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f17ef79512c248af88a6336aae5d73f5-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,89.5 395.0,89.5 395.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f17ef79512c248af88a6336aae5d73f5-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f17ef79512c248af88a6336aae5d73f5-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f17ef79512c248af88a6336aae5d73f5-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f17ef79512c248af88a6336aae5d73f5-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f17ef79512c248af88a6336aae5d73f5-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M565.0,266.5 L573.0,254.5 557.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f17ef79512c248af88a6336aae5d73f5-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f17ef79512c248af88a6336aae5d73f5-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M740.0,266.5 L748.0,254.5 732.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f17ef79512c248af88a6336aae5d73f5-0-4\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f17ef79512c248af88a6336aae5d73f5-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,266.5 L937,254.5 953,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f17ef79512c248af88a6336aae5d73f5-0-5\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,177.0 1265.0,177.0 1265.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f17ef79512c248af88a6336aae5d73f5-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,266.5 L1112,254.5 1128,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f17ef79512c248af88a6336aae5d73f5-0-6\" stroke-width=\"2px\" d=\"M770,264.5 C770,2.0 1275.0,2.0 1275.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f17ef79512c248af88a6336aae5d73f5-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1275.0,266.5 L1283.0,254.5 1267.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f17ef79512c248af88a6336aae5d73f5-0-7\" stroke-width=\"2px\" d=\"M1295,264.5 C1295,177.0 1440.0,177.0 1440.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f17ef79512c248af88a6336aae5d73f5-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1440.0,266.5 L1448.0,254.5 1432.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f17ef79512c248af88a6336aae5d73f5-0-8\" stroke-width=\"2px\" d=\"M1645,264.5 C1645,89.5 1970.0,89.5 1970.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f17ef79512c248af88a6336aae5d73f5-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1645,266.5 L1637,254.5 1653,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f17ef79512c248af88a6336aae5d73f5-0-9\" stroke-width=\"2px\" d=\"M1820,264.5 C1820,177.0 1965.0,177.0 1965.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f17ef79512c248af88a6336aae5d73f5-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1820,266.5 L1812,254.5 1828,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-f17ef79512c248af88a6336aae5d73f5-0-10\" stroke-width=\"2px\" d=\"M1470,264.5 C1470,2.0 1975.0,2.0 1975.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-f17ef79512c248af88a6336aae5d73f5-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1975.0,266.5 L1983.0,254.5 1967.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style = 'dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bfc2f23a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"c4ac898bfba54d60a3d8e17192896215-0\" class=\"displacy\" width=\"1250\" height=\"287.0\" direction=\"ltr\" style=\"max-width: none; height: 287.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Apple</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"150\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"150\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"250\">looking</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"250\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">buying</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"550\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"550\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">U.K.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">startup</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"850\">for</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"850\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">$</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">SYM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1050\">1</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1050\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"197.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1150\">billion</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1150\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c4ac898bfba54d60a3d8e17192896215-0-0\" stroke-width=\"2px\" d=\"M62,152.0 62,118.66666666666666 247.0,118.66666666666666 247.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c4ac898bfba54d60a3d8e17192896215-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M62,154.0 L58,146.0 66,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c4ac898bfba54d60a3d8e17192896215-0-1\" stroke-width=\"2px\" d=\"M162,152.0 162,135.33333333333334 244.0,135.33333333333334 244.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c4ac898bfba54d60a3d8e17192896215-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M162,154.0 L158,146.0 166,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c4ac898bfba54d60a3d8e17192896215-0-2\" stroke-width=\"2px\" d=\"M262,152.0 262,135.33333333333334 344.0,135.33333333333334 344.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c4ac898bfba54d60a3d8e17192896215-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M344.0,154.0 L348.0,146.0 340.0,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c4ac898bfba54d60a3d8e17192896215-0-3\" stroke-width=\"2px\" d=\"M362,152.0 362,135.33333333333334 444.0,135.33333333333334 444.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c4ac898bfba54d60a3d8e17192896215-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M444.0,154.0 L448.0,146.0 440.0,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c4ac898bfba54d60a3d8e17192896215-0-4\" stroke-width=\"2px\" d=\"M562,152.0 562,135.33333333333334 644.0,135.33333333333334 644.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c4ac898bfba54d60a3d8e17192896215-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M562,154.0 L558,146.0 566,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c4ac898bfba54d60a3d8e17192896215-0-5\" stroke-width=\"2px\" d=\"M662,152.0 662,135.33333333333334 744.0,135.33333333333334 744.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c4ac898bfba54d60a3d8e17192896215-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M662,154.0 L658,146.0 666,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c4ac898bfba54d60a3d8e17192896215-0-6\" stroke-width=\"2px\" d=\"M462,152.0 462,102.0 750.0,102.0 750.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c4ac898bfba54d60a3d8e17192896215-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M750.0,154.0 L754.0,146.0 746.0,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c4ac898bfba54d60a3d8e17192896215-0-7\" stroke-width=\"2px\" d=\"M762,152.0 762,135.33333333333334 844.0,135.33333333333334 844.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c4ac898bfba54d60a3d8e17192896215-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M844.0,154.0 L848.0,146.0 840.0,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c4ac898bfba54d60a3d8e17192896215-0-8\" stroke-width=\"2px\" d=\"M962,152.0 962,118.66666666666666 1147.0,118.66666666666666 1147.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c4ac898bfba54d60a3d8e17192896215-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">quantmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M962,154.0 L958,146.0 966,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c4ac898bfba54d60a3d8e17192896215-0-9\" stroke-width=\"2px\" d=\"M1062,152.0 1062,135.33333333333334 1144.0,135.33333333333334 1144.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c4ac898bfba54d60a3d8e17192896215-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1062,154.0 L1058,146.0 1066,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-c4ac898bfba54d60a3d8e17192896215-0-10\" stroke-width=\"2px\" d=\"M862,152.0 862,102.0 1150.0,102.0 1150.0,152.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-c4ac898bfba54d60a3d8e17192896215-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1150.0,154.0 L1154.0,146.0 1146.0,146.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc, style = 'dep' , options = {'distance': 100 , 'compact' : True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4ac0c8",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15ac631a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "U.K. GPE\n",
      "$1 billion MONEY\n"
     ]
    }
   ],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text , ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5db4b41b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is looking for buying a \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    U.K.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " startup for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $1 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(doc , style = 'ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed80b9c0",
   "metadata": {},
   "source": [
    "# Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aebcb1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Samsung is looking to buy a U.K. startup. Government says that it may have the permission! Crazy, Right?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea7fc6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df6b7629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samsung is looking to buy a U.K. startup.\n",
      "Government says that it may have the permission!\n",
      "Crazy, Right?\n"
     ]
    }
   ],
   "source": [
    "for sent in doc.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed51c852",
   "metadata": {},
   "source": [
    "# Rule Based Phrase Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b1bfceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "53d293bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Hello World! hello world'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b7727a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2e56278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      "World\n",
      "!\n",
      "hello\n",
      "world\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d64e0abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{'LOWER' :'hello'} , {'IS_PUNCT' : True} , {'LOWER' : 'world'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "40dbfb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function add:\n",
      "\n",
      "add(...) method of spacy.matcher.matcher.Matcher instance\n",
      "    Matcher.add(self, key, patterns, *, on_match=None, greedy: str = None)\n",
      "    Add a match-rule to the matcher. A match-rule consists of: an ID\n",
      "            key, an on_match callback, and one or more patterns.\n",
      "    \n",
      "            If the key exists, the patterns are appended to the previous ones, and\n",
      "            the previous on_match callback is replaced. The `on_match` callback\n",
      "            will receive the arguments `(matcher, doc, i, matches)`. You can also\n",
      "            set `on_match` to `None` to not perform any actions.\n",
      "    \n",
      "            A pattern consists of one or more `token_specs`, where a `token_spec`\n",
      "            is a dictionary mapping attribute IDs to values, and optionally a\n",
      "            quantifier operator under the key \"op\". The available quantifiers are:\n",
      "    \n",
      "            '!': Negate the pattern, by requiring it to match exactly 0 times.\n",
      "            '?': Make the pattern optional, by allowing it to match 0 or 1 times.\n",
      "            '+': Require the pattern to match 1 or more times.\n",
      "            '*': Allow the pattern to zero or more times.\n",
      "    \n",
      "            The + and * operators return all possible matches (not just the greedy\n",
      "            ones). However, the \"greedy\" argument can filter the final matches\n",
      "            by returning a non-overlapping set per key, either taking preference to\n",
      "            the first greedy match (\"FIRST\"), or the longest (\"LONGEST\").\n",
      "    \n",
      "            Since spaCy v2.2.2, Matcher.add takes a list of patterns as the second\n",
      "            argument, and the on_match callback is an optional keyword argument.\n",
      "    \n",
      "            key (Union[str, int]): The match ID.\n",
      "            patterns (list): The patterns to add for the given key.\n",
      "            on_match (callable): Optional callback executed on match.\n",
      "            greedy (str): Optional filter: \"FIRST\" or \"LONGEST\".\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(matcher.add)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6ebb12c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add('hw' ,  [pattern]) # id , pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "71231122",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "16fff6d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628c0954",
   "metadata": {},
   "source": [
    "# Processing Pipeline in Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9901838b",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    'net income was $9.4 million usd compared to 2.7 million dollars ',\n",
    "    'revenue exceeds 12 million dollars in 2025 with a gain of 700% and valuation at $2.7b'\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e7443c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/heisenberg/anaconda3/envs/tf22_gpu/lib/python3.8/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$9.4 million MONEY\n",
      "2.7 million dollars MONEY\n",
      "---------------------------------------\n",
      "12 million dollars MONEY\n",
      "2025 DATE\n",
      "700% PERCENT\n",
      "2.7b MONEY\n",
      "---------------------------------------\n",
      "$9.4 million MONEY\n",
      "2.7 million dollars MONEY\n",
      "---------------------------------------\n",
      "12 million dollars MONEY\n",
      "2025 DATE\n",
      "700% PERCENT\n",
      "2.7b MONEY\n",
      "---------------------------------------\n",
      "$9.4 million MONEY\n",
      "2.7 million dollars MONEY\n",
      "---------------------------------------\n",
      "12 million dollars MONEY\n",
      "2025 DATE\n",
      "700% PERCENT\n",
      "2.7b MONEY\n",
      "---------------------------------------\n",
      "$9.4 million MONEY\n",
      "2.7 million dollars MONEY\n",
      "---------------------------------------\n",
      "12 million dollars MONEY\n",
      "2025 DATE\n",
      "700% PERCENT\n",
      "2.7b MONEY\n",
      "---------------------------------------\n",
      "$9.4 million MONEY\n",
      "2.7 million dollars MONEY\n",
      "---------------------------------------\n",
      "12 million dollars MONEY\n",
      "2025 DATE\n",
      "700% PERCENT\n",
      "2.7b MONEY\n",
      "---------------------------------------\n",
      "$9.4 million MONEY\n",
      "2.7 million dollars MONEY\n",
      "---------------------------------------\n",
      "12 million dollars MONEY\n",
      "2025 DATE\n",
      "700% PERCENT\n",
      "2.7b MONEY\n",
      "---------------------------------------\n",
      "$9.4 million MONEY\n",
      "2.7 million dollars MONEY\n",
      "---------------------------------------\n",
      "12 million dollars MONEY\n",
      "2025 DATE\n",
      "700% PERCENT\n",
      "2.7b MONEY\n",
      "---------------------------------------\n",
      "$9.4 million MONEY\n",
      "2.7 million dollars MONEY\n",
      "---------------------------------------\n",
      "12 million dollars MONEY\n",
      "2025 DATE\n",
      "700% PERCENT\n",
      "2.7b MONEY\n",
      "---------------------------------------\n",
      "864 ms ± 187 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "docs = nlp.pipe(texts , disable = ['tagger' , 'parser'])\n",
    "for doc in docs:\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text , ent.label_)\n",
    "    print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "82a1288b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$9.4 million MONEY\n",
      "2.7 million dollars MONEY\n",
      "---------------------------------------\n",
      "12 million dollars MONEY\n",
      "2025 DATE\n",
      "700% PERCENT\n",
      "2.7b MONEY\n",
      "---------------------------------------\n",
      "$9.4 million MONEY\n",
      "2.7 million dollars MONEY\n",
      "---------------------------------------\n",
      "12 million dollars MONEY\n",
      "2025 DATE\n",
      "700% PERCENT\n",
      "2.7b MONEY\n",
      "---------------------------------------\n",
      "$9.4 million MONEY\n",
      "2.7 million dollars MONEY\n",
      "---------------------------------------\n",
      "12 million dollars MONEY\n",
      "2025 DATE\n",
      "700% PERCENT\n",
      "2.7b MONEY\n",
      "---------------------------------------\n",
      "$9.4 million MONEY\n",
      "2.7 million dollars MONEY\n",
      "---------------------------------------\n",
      "12 million dollars MONEY\n",
      "2025 DATE\n",
      "700% PERCENT\n",
      "2.7b MONEY\n",
      "---------------------------------------\n",
      "$9.4 million MONEY\n",
      "2.7 million dollars MONEY\n",
      "---------------------------------------\n",
      "12 million dollars MONEY\n",
      "2025 DATE\n",
      "700% PERCENT\n",
      "2.7b MONEY\n",
      "---------------------------------------\n",
      "$9.4 million MONEY\n",
      "2.7 million dollars MONEY\n",
      "---------------------------------------\n",
      "12 million dollars MONEY\n",
      "2025 DATE\n",
      "700% PERCENT\n",
      "2.7b MONEY\n",
      "---------------------------------------\n",
      "$9.4 million MONEY\n",
      "2.7 million dollars MONEY\n",
      "---------------------------------------\n",
      "12 million dollars MONEY\n",
      "2025 DATE\n",
      "700% PERCENT\n",
      "2.7b MONEY\n",
      "---------------------------------------\n",
      "$9.4 million MONEY\n",
      "2.7 million dollars MONEY\n",
      "---------------------------------------\n",
      "12 million dollars MONEY\n",
      "2025 DATE\n",
      "700% PERCENT\n",
      "2.7b MONEY\n",
      "---------------------------------------\n",
      "763 ms ± 51.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "docs = nlp.pipe(texts)\n",
    "for doc in docs:\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text , ent.label_)\n",
    "    print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9f0ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf22_gpu] *",
   "language": "python",
   "name": "conda-env-tf22_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
